{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acd2de48-cfdb-4cf4-be22-66253ef26af5",
   "metadata": {},
   "source": [
    "# Analyzing ICESat-2 snow depths over Alaska\n",
    "\n",
    "This notebook is designed to take the frameworks from the SlideRule and icepyx querying notebooks to examine ICESat-2 snow depths over SnowEx field sites in Alaska. \n",
    "\n",
    "Snow-off/-on lidar DEMs from the UAF lidar are required for the running of this script. Note that the airborne lidar data is still preliminary, so it is currently not available publicly. Until the data is posted to NSIDC, it is recommended for any users to reach out to the original provider of the data (Chris Larsen, cflarsen@uaf.edu), and be aware that there may be a few oddities in the DEMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fbad61-89a2-4250-9690-82b51d619a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import geopandas as gpd\n",
    "import holoviews as hv\n",
    "from holoviews import opts, Cycle\n",
    "import icepyx as ipx\n",
    "from is2_cloud_access import atl03q, atl06q, atl08q\n",
    "import lidar_processing as lp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyproj import Proj, transform, Transformer, CRS\n",
    "import rioxarray as rio\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Polygon, Point\n",
    "import sys\n",
    "import xarray as xr\n",
    "hv.extension('bokeh')\n",
    "\n",
    "# SlideRule-relevant packages\n",
    "import ipywidgets as widgets\n",
    "import logging\n",
    "import concurrent.futures\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sliderule import icesat2\n",
    "from sliderule import sliderule, ipysliderule, io\n",
    "import geoviews as gv\n",
    "import geoviews.feature as gf\n",
    "from geoviews import dim, opts\n",
    "import geoviews.tile_sources as gts\n",
    "from bokeh.models import HoverTool\n",
    "import hvplot.pandas\n",
    "gv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b95778-d39d-4726-8ff5-36d4caa32a62",
   "metadata": {},
   "source": [
    "### User Input\n",
    "\n",
    "Acceptable field site IDs over Alaska are:\n",
    "* 'cffl': Creamer's Field/Farmer's Loop\n",
    "* 'cpcrw': Caribou/Poker Creek Experimental Watershed\n",
    "* 'bcef': Bonanza Creek Experimental Forest\n",
    "* 'trs': Toolik Research Station\n",
    "* 'acp': Arctic Coastal Plain (Deadhorse)\n",
    "\n",
    "**NOTE**: Functionality for Caribou/Poker Creek and Toolik data is a work in progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2813b2-fc97-4735-87ad-6f04466b2425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field site ID\n",
    "field_id = 'acp'\n",
    "\n",
    "# Snow-on (True) or snow-off (False) analysis\n",
    "snow_on = True\n",
    "\n",
    "# Base data path\n",
    "path = '/home/jovyan/icesat2-snowex/'\n",
    "\n",
    "# Desired RGT and date range for queries (needed to filter SlideRule and for icepyx query)\n",
    "date_range = ['2022-03-01', '2022-04-30']\n",
    "rgt = '1097'\n",
    "\n",
    "# Save the data to a CSV?\n",
    "saving = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894de7bc-79a8-4f3e-89a4-ce1d07da1dbe",
   "metadata": {},
   "source": [
    "### Read UAF Lidar Data\n",
    "\n",
    "Note that the snow-off lidar DEM is needed to estimate ICESat-2 snow depths, so it must be available even for snow-on analyses.\n",
    "\n",
    "A correction factor is applied to the snow-off DEM to account for vertical datum differences. It is currently field-site dependent until a more robust calibration is derived.\n",
    "\n",
    "Field site data will be added to this segment when it becomes available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f7dbe5-30e9-4f13-9647-eca9ee86f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if field_id == 'cffl':\n",
    "    f_snow_off = '%s/lidar-dems/farmersloop_2022may28_dtm_3m.tif' %(path)\n",
    "    f_snow_on = '%s/lidar-dems/farmersloop_2022mar11_snowdepth_3m.tif' %(path)\n",
    "    \n",
    "    # Vertical datum correction factor\n",
    "    lidar_correction_factor = 9.75\n",
    "    \n",
    "elif field_id == 'bcef':\n",
    "    f_snow_off = '%s/lidar-dems/bonanza_2022may28_dtm_3m.tif' %(path)\n",
    "    f_snow_on = '%s/lidar-dems/bonanza_2022mar11_snowdepth_3m.tif' %(path)\n",
    "    \n",
    "    # Vertical datum correction factor\n",
    "    lidar_correction_factor = 9.9\n",
    "    \n",
    "elif field_id == 'acp':\n",
    "    f_snow_off = '%s/lidar-dems/coastalplain_2022aug31_dtm_3m.tif' %(path)\n",
    "    f_snow_on = '%s/lidar-dems/coastalplain_2022mar12_snowdepth_3m.tif' %(path)\n",
    "    \n",
    "    # Vertical datum correction factor\n",
    "    lidar_correction_factor = 9.9\n",
    "    print('CAUTION: ICESat-2/UAF calibration has not been performed for this field site. Snow-off results may be inaccurate.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfdf5d1-a4f8-4e40-8be2-06917327401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read lidar DEMs into xarray (rasterio) format\n",
    "lidar_snow_off = rio.open_rasterio(f_snow_off)\n",
    "lidar_snow_on = rio.open_rasterio(f_snow_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb958df-ed1b-400f-b4f7-8c180ce06c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_snow_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2754854a-0088-4f0e-86c8-ffadc6066ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_snow_on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3ab4ec-9f05-4bfb-9ab1-72de79fa29d9",
   "metadata": {},
   "source": [
    "### Read ICESat-2 Data\n",
    "\n",
    "Again, reading of the ICESat-2 data will be site specific. These cells will be reading in **ATL03, ATL06, and ATL08** data. ATL03 will be accessed and processed using SlideRule, whereas icepyx will provide ATL06 and ATL08. Both of these software packages will consolidate the ICESat-2 products under the hood, for cleanliness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece18bab-47e0-406c-89aa-4b51e669de42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the ICESat-2 data products\n",
    "atl03 = atl03q(field_id)\n",
    "atl06 = atl06q(field_id, date_range, rgt)\n",
    "atl08 = atl08q(field_id, date_range, rgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad8cb2a-203e-4362-8977-be4351968dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "atl03.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4279a03f-1b81-4425-aa7d-7c7b36f120dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "atl06.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62578d6-6980-44d4-b582-6cb6abbb8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "atl08.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495d948c-af6a-40d6-abe8-9b3403e7f018",
   "metadata": {},
   "source": [
    "### Processing ICESat-2 data\n",
    "\n",
    "Now that we have batches of ICESat-2 data for ATL03, ATL06, and ATL08, we will now do some filtering to only look at our RGT, time, and region of interest in all three products. We will also add easting/northing coordinates to each DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab797f0f-fcad-4d33-b0dd-e32414e008ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add easting/northing coordinates to ATL03 DataFrame\n",
    "atl03['x'] = atl03.to_crs(epsg=32606).geometry.x\n",
    "atl03['y'] = atl03.to_crs(epsg=32606).geometry.y\n",
    "\n",
    "atl03.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0193c5be-28ce-43f4-ae84-e3edc10eec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add easting/northing coordinates to ATL06/08 DataFrames\n",
    "#inp = Proj('epsg:4326')\n",
    "#outp = Proj('epsg:32606')\n",
    "\n",
    "transformer = Transformer.from_crs('EPSG:4326', 'EPSG:32606', always_xy=True)\n",
    "\n",
    "atl06['x'], atl06['y'] = transformer.transform(atl06.lon, atl06.lat)\n",
    "atl08['x'], atl08['y'] = transformer.transform(atl08.lon, atl08.lat)\n",
    "\n",
    "print(atl06.head())\n",
    "print(atl08.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daf1549-a546-47dd-a69e-8b36d30236d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter ATL03 data by RGT and date range\n",
    "atl03 = atl03.loc[atl03['rgt'] == int(rgt)]\n",
    "atl03 = atl03[atl03.index.to_series().between(date_range[0], date_range[1])]\n",
    "atl03.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b013bd-ed05-43ab-bef3-5b673c174f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include only the strong beams in the ATL03 DataFrame\n",
    "strong_spots = ['1', '3', '5']\n",
    "atl03['spot'] = atl03['spot'].apply(str)\n",
    "atl03 = atl03[atl03['spot'].isin(strong_spots)]\n",
    "\n",
    "# Change the ATL03 height column name to be consistent with other DataFrames\n",
    "atl03 = atl03.rename(columns={'h_mean': 'height'})\n",
    "atl03.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae95a5-d9c5-4323-b71a-237540a57ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove filler/messy data from ATL06/08\n",
    "# ATL06\n",
    "upper = atl06.height.mean() + 3*atl06.height.std()\n",
    "atl06 = atl06.loc[atl06.height<upper]\n",
    "\n",
    "# ATL08\n",
    "upper = atl08.height.mean() + 3*atl08.height.std()\n",
    "atl08 = atl08.loc[atl08.height<upper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096a076c-8bc9-48cf-bb6a-ee64bb7e2b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit ATL06/08 to ATL03 spatial bounds\n",
    "atl06 = atl06[(atl06.y.values>atl03.y.min()) & (atl06.y.values<atl03.y.max())]\n",
    "atl08 = atl08[(atl08.y.values>atl03.y.min()) & (atl08.y.values<atl03.y.max())]\n",
    "\n",
    "print(atl06.head())\n",
    "print(atl08.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f61b09-d25b-4d28-84a9-9f32c4bd7cbb",
   "metadata": {},
   "source": [
    "### Process UAF Lidar Data\n",
    "\n",
    "Here, we are going to apply the lidar correction factor to the snow-off lidar data, then coregister the DEMs with each of the ICESat-2 data products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f02b92-252a-42c8-9f5c-42879c8a4032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply vertical datum correction\n",
    "lidar_snow_off -= lidar_correction_factor\n",
    "\n",
    "# Coregister with ICESat-2 using a bivariate spline\n",
    "strong_ids = np.unique(atl06['gt'])\n",
    "atl03_uaf = lp.coregister_is2(lidar_snow_off, lidar_snow_on, atl03, strong_ids)\n",
    "atl06_uaf = lp.coregister_is2(lidar_snow_off, lidar_snow_on, atl06, strong_ids)\n",
    "atl08_uaf = lp.coregister_is2(lidar_snow_off, lidar_snow_on, atl08, strong_ids)\n",
    "\n",
    "# Calculate snow depth residuals\n",
    "if snow_on:\n",
    "    atl03_uaf['snow_depth_residual'] = atl03_uaf['residual'] - atl03_uaf['lidar_snow_depth']\n",
    "    atl06_uaf['snow_depth_residual'] = atl06_uaf['residual'] - atl06_uaf['lidar_snow_depth']\n",
    "    atl08_uaf['snow_depth_residual'] = atl08_uaf['residual'] - atl08_uaf['lidar_snow_depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d98d83e-3554-4cbb-aa10-05b6401ebc63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(atl03_uaf.head())\n",
    "print(atl06_uaf.head())\n",
    "print(atl08_uaf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa2631f-2e4c-47a3-8e02-ae695b809b4c",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Statistical Filtering\n",
    "\n",
    "The interpolation included some messy data or filler values, so this is a short section to only include data within the 10th and 90th percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69346378-ac29-4f6e-8b5f-e4f6f4a35bbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ATL03\n",
    "upper = atl03_uaf['residual'].quantile(0.9)\n",
    "lower = atl03_uaf['residual'].quantile(0.1)\n",
    "atl03_uaf_filtered = atl03_uaf[(atl03_uaf['residual']>=lower) &\n",
    "                               (atl03_uaf['residual']<=upper) &\n",
    "                               (atl03_uaf['lidar_snow_depth']>0)]\n",
    "\n",
    "# ATL06\n",
    "upper = atl06_uaf['residual'].quantile(0.9)\n",
    "lower = atl06_uaf['residual'].quantile(0.1)\n",
    "atl06_uaf_filtered = atl06_uaf[(atl06_uaf['residual']>=lower) &\n",
    "                               (atl06_uaf['residual']<=upper) &\n",
    "                               (atl06_uaf['lidar_snow_depth']>0)]\n",
    "\n",
    "# ATL08\n",
    "upper = atl08_uaf['residual'].quantile(0.9)\n",
    "lower = atl08_uaf['residual'].quantile(0.1)\n",
    "atl08_uaf_filtered = atl08_uaf[(atl08_uaf['residual']>=lower) &\n",
    "                               (atl08_uaf['residual']<=upper) &\n",
    "                               (atl08_uaf['lidar_snow_depth']>0)]\n",
    "\n",
    "print(atl03_uaf_filtered.head())\n",
    "print(atl06_uaf_filtered.head())\n",
    "print(atl08_uaf_filtered.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b6e6d1-79d6-4633-9b22-82434b38f916",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Saving snow depth data\n",
    "The final aim for this notebook is to develop an ICESat-2 snow depth \"product\" over the Alaska field sites. This is still very much a work in progress, but the section still allows for saving of derived snow depth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f848d7e-1ab6-4bac-a554-500c8db0f446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save derived snow depth data in .csv format\n",
    "atl03_uaf_filtered.to_csv(f'atl03_snowdepth_rgt{rgt}_{field_id}_{time_str}.csv')\n",
    "atl06_uaf_filtered.to_csv(f'atl06_snowdepth_rgt{rgt}_{field_id}_{time_str}.csv')\n",
    "atl08_uaf_filtered.to_csv(f'atl08_snowdepth_rgt{rgt}_{field_id}_{time_str}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cccd7dc-f9f6-49e1-89bd-356a06f880c9",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Plotting\n",
    "\n",
    "Now that we have processed and co-registered the two lidar data sets, let's make some plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7778cf-6a96-4bf8-90bb-aa1e2a691cfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add latitude, longitude coordinates to DataFrames\n",
    "transformer = Transformer.from_crs('EPSG:32606', 'EPSG:4326', always_xy=True)\n",
    "\n",
    "atl03_uaf_filtered['lon'], atl03_uaf_filtered['lat'] = transformer.transform(atl03_uaf_filtered.x,\n",
    "                                                                             atl03_uaf_filtered.y)\n",
    "atl06_uaf_filtered['lon'], atl06_uaf_filtered['lat'] = transformer.transform(atl06_uaf_filtered.x,\n",
    "                                                                             atl06_uaf_filtered.y)\n",
    "atl08_uaf_filtered['lon'], atl08_uaf_filtered['lat'] = transformer.transform(atl08_uaf_filtered.x,\n",
    "                                                                             atl08_uaf_filtered.y)\n",
    "\n",
    "# Convert DataFrames to GeoDataFrames\n",
    "# ATL03\n",
    "geometry = [Point(xy) for xy in zip(atl03_uaf_filtered.lon, atl03_uaf_filtered.lat)]\n",
    "atl03_uaf_gpd = gpd.GeoDataFrame(atl03_uaf_filtered, geometry=geometry)\n",
    "\n",
    "# ATL06\n",
    "geometry = [Point(xy) for xy in zip(atl06_uaf_filtered.lon, atl06_uaf_filtered.lat)]\n",
    "atl06_uaf_gpd = gpd.GeoDataFrame(atl06_uaf_filtered, geometry=geometry)\n",
    "\n",
    "# ATL08\n",
    "geometry = [Point(xy) for xy in zip(atl08_uaf_filtered.lon, atl08_uaf_filtered.lat)]\n",
    "atl08_uaf_gpd = gpd.GeoDataFrame(atl08_uaf_filtered, geometry=geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc6b5b7-14fe-49bb-ac82-b7688fb46fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in the SnowEx lidar boxes\n",
    "lidar_boxes = gpd.read_file('/home/jovyan/icesat2-snowex/jsons-shps/snowex_lidar_swaths.shp')\n",
    "\n",
    "# Geoviews map of snow depth over site\n",
    "hover = HoverTool(tooltips=[('Latitude', '@Latitude'),\n",
    "                             ('Longitude', '@Longitude'),\n",
    "                             ('is2_snow_depth', '@residual')])\n",
    "lidar_boxes_poly = gv.Polygons(lidar_boxes).opts(color='white',\n",
    "                                                 alpha=0.5)\n",
    "points_on_map = gv.Points(atl03_uaf_gpd,\n",
    "                           kdmins=['Longitude', 'Latitude'],\n",
    "                           vdmins=['residual']).opts(tools=[hover],\n",
    "                                                     color_index='residual',\n",
    "                                                     colorbar=True,\n",
    "                                                     clabel='IS-2 snow depth [m]',\n",
    "                                                     size=4.0)\n",
    "\n",
    "\n",
    "world_map = gts.EsriImagery.opts(width=600, height=570) * gts.StamenLabels.options(level='annotation')\n",
    "\n",
    "world_map * lidar_boxes_poly * points_on_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e5306c-3787-48b5-92c3-c1604c2703e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate histogram bins for ICESat-2 snow depth data\n",
    "freq, edges = np.histogram(atl03_uaf_filtered['residual'], 100)\n",
    "freq06, edges06 = np.histogram(atl06_uaf_filtered['residual'], 100)\n",
    "freq08, edges08 = np.histogram(atl08_uaf_filtered['residual'], 100)\n",
    "frequaf, edgesuaf = np.histogram(atl03_uaf_filtered['lidar_snow_depth'][abs(atl03_uaf_filtered['lidar_snow_depth'])<3], 100)\n",
    "\n",
    "# Generate scatter plot for UAF vs. ICESat-2 snow depth\n",
    "scatter = atl03_uaf_filtered.hvplot(kind='scatter', x='lidar_snow_depth', y='residual', label='ATL03', alpha=0.5)\n",
    "scatter06 = atl06_uaf_filtered.hvplot(kind='scatter', x='lidar_snow_depth', y='residual', label='ATL06', alpha=0.5)\n",
    "scatter08 = atl08_uaf_filtered.hvplot(kind='scatter', x='lidar_snow_depth', y='residual', label='ATL08', alpha=0.5)\n",
    "scatters = (scatter*scatter06*scatter08).opts(xlabel='UAF snow depth [m]', ylabel='ICESat-2 snow depth [m]')\n",
    "\n",
    "# Generate PDF curves for snow depth\n",
    "curves = (hv.Distribution((edges, freq), label='ATL03').opts(bandwidth=0.6)) * \\\n",
    "         (hv.Distribution((edges06, freq06), label='ATL06').opts(bandwidth=0.6)) * \\\n",
    "         (hv.Distribution((edges08, freq08), label='ATL08').opts(bandwidth=0.6)) * \\\n",
    "         (hv.Distribution((edgesuaf, frequaf), label='UAF').opts(bandwidth=0.6))\n",
    "curves.opts(xlabel='Snow depth [m]', ylabel='PDF')\n",
    "\n",
    "scatters + curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28cbe0a-aa61-494e-b519-6bc8a2d0e1b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f240f2-eaf9-4f9a-ad68-f468bd505f4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f355314-556c-4d35-9122-b12244b7530b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
